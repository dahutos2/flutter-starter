## UC01 画像と結果の文字列のペアをまとめて登録し、学習を実行する

### 1. ローカルから CSV ファイルを S3 にアップロードするバッチスクリプト

まず、ローカルから CSV ファイルを S3 にアップロードするバッチスクリプトを作成します。

#### upload_to_s3.bat

```batch
@echo off
setlocal

REM S3バケット名とCSVファイルのパスを設定
set AWS_BUCKET=ogiri-training-data-bucket
set CSV_FILE=path\to\your\train_data.csv

REM CSVファイルをS3にアップロード
aws s3 cp %CSV_FILE% s3://%AWS_BUCKET%/

endlocal
```

### 2. IAM ポリシーの作成

#### 2.1. トレーニングジョブ開始する Lambda 用のポリシーの作成

##### 手順:

1. **AWS マネジメントコンソール**にログインし、**IAM**ダッシュボードに移動します。
2. 左側のメニューから「**ポリシー**」を選択し、「**ポリシーを作成**」ボタンをクリックします。
3. **JSON**タブを選択し、以下のポリシーを貼り付けます:

<details><summary>詳細を開く</summary>

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:PutObject", "s3:GetObject"],
      "Resource": ["arn:aws:s3:::ogiri-training-data-bucket/*"]
    },
    {
      "Effect": "Allow",
      "Action": ["dynamodb:GetItem", "dynamodb:PutItem", "dynamodb:PutItem"],
      "Resource": [
        "arn:aws:dynamodb:ap-northeast-1:765231401377:table/OgiriTrainingDataTable"
      ]
    },
    {
      "Effect": "Allow",
      "Action": ["rekognition:DetectLabels"],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": ["sagemaker:CreateTrainingJob"],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": ["logs:CreateLogGroup", "logs:CreateLogStream", "logs:PutLogEvents"],
      "Resource": ["arn:aws:logs:ap-northeast-1:765231401377:log-group:/aws/lambda/*"]
    }
  ]
}
```

</details>

4. 「**次のステップ: タグ**」をクリックし、任意のタグを追加します（省略可能）。
5. 「**次のステップ: 確認**」をクリックし、ポリシーに名前（例: `LambdaStartOgiriTrainingJobPolicy`）と説明を入力して「**ポリシーの作成**」をクリックします。

#### 2.2. トレーニングジョブ終了後の Lambda 用のポリシーの作成

##### 手順:

1. [2.1](#21-トレーニングジョブ開始するlambda用のポリシーの作成)同様の手順で以下のポリシー（`LambdaEndOgiriTrainingJobPolicy`）を作成する

<details><summary>詳細を開く</summary>

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "sagemaker:CreateModel",
        "sagemaker:CreateEndpointConfig",
        "sagemaker:CreateEndpoint",
        "sagemaker:DescribeTrainingJob"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": ["logs:CreateLogGroup", "logs:CreateLogStream", "logs:PutLogEvents"],
      "Resource": ["arn:aws:logs:ap-northeast-1:765231401377:log-group:/aws/lambda/*"]
    }
  ]
}
```

</details>

### 3. IAM ロールの作成

#### 3.1. トレーニングジョブ開始する Lambda 用のロールの作成

##### 手順:

1. IAM ダッシュボードの左側のメニューから「**ロール**」を選択し、「**ロールを作成**」ボタンをクリックします。
2. 「**信頼されたエンティティのタイプを選択**」画面で「**AWS サービス**」を選択し、「**Lambda**」を選択します。「**次のステップ**」をクリックします。
3. 先ほど作成したポリシー（`LambdaStartOgiriTrainingJobPolicy`）を検索し、選択して「**次のステップ**」をクリックします。
4. ロールに名前（例: `LambdaStartOgiriTrainingJobRole`）を付けて「**ロールの作成**」をクリックします。

#### 3.2. トレーニングジョブ終了後の Lambda 用のロールの作成

##### 手順:

1. [3.1](#31-トレーニングジョブ開始するlambda用のロールの作成)と同様の手順で以下の名前とポリシーのロールを作成する
   - ポリシー名: `LambdaEndOgiriTrainingJobPolicy`
   - ロール名: `LambdaEndOgiriTrainingJobRole`

#### 3.3. SageMaker 用のロールの作成

##### 手順:

1. IAM ダッシュボードの左側のメニューから「**ロール**」を選択し、「**ロールを作成**」ボタンをクリックします。
2. 「**信頼されたエンティティのタイプを選択**」画面で「**AWS サービス**」を選択し、「**SageMaker**」を選択します。「**次のステップ**」をクリックします。
3. デフォルトポリシー（`AmazonSageMakerFullAccess`）を選択して「**次のステップ**」をクリックします。
4. ロールに名前（例: `SageMakerOgiriTrainingJobRole`）を付けて「**ロールの作成**」をクリックします。

### 4. IAM ユーザーに権限を追加

#### 4.1. IAM ユーザーに必要なポリシーを作成

1. **AWS マネジメントコンソール**にログインし、**IAM**ダッシュボードに移動します。
2. 左側のメニューから「**ポリシー**」を選択し、「**ポリシーを作成**」ボタンをクリックします。
3. **JSON**タブを選択し、以下のポリシーを貼り付けます:

<details><summary>詳細を開く</summary>

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:ListBucket", "s3:GetObject", "s3:PutObject", "s3:DeleteObject"],
      "Resource": [
        "arn:aws:s3:::ogiri-training-data-bucket",
        "arn:aws:s3:::ogiri-training-data-bucket/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": "rekognition:DetectLabels",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "dynamodb:PutItem",
        "dynamodb:GetItem",
        "dynamodb:UpdateItem",
        "dynamodb:Scan",
        "dynamodb:Query"
      ],
      "Resource": "arn:aws:dynamodb:ap-northeast-1:765231401377:table/OgiriTrainingDataTable"
    },
    {
      "Effect": "Allow",
      "Action": [
        "sagemaker:CreateTrainingJob",
        "sagemaker:DescribeTrainingJob",
        "sagemaker:CreateModel",
        "sagemaker:CreateEndpointConfig",
        "sagemaker:CreateEndpoint",
        "sagemaker:InvokeEndpoint"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": ["logs:CreateLogGroup", "logs:CreateLogStream", "logs:PutLogEvents"],
      "Resource": "arn:aws:logs:ap-northeast-1:765231401377:log-group:/aws/lambda/*"
    },
    {
      "Effect": "Allow",
      "Action": ["lambda:InvokeFunction"],
      "Resource": "arn:aws:lambda:ap-northeast-1:765231401377:function:StartOgiriTrainingJob"
    }
  ]
}
```

</details>

4. 「**次のステップ: タグ**」をクリックし、任意のタグを追加します（省略可能）。
5. 「**次のステップ: 確認**」をクリックし、ポリシーに名前（例: `UserOgiriTrainingJobPolicy`）と説明を入力して「**ポリシーの作成**」をクリックします。

#### 4.2. IAM ユーザーに必要なポリシーを追加

1. 左側のメニューから「**ユーザー**」を選択し、作成済みのユーザのリンクをクリックします。
2. 「**許可**」タブの「**許可ポリシー**」画面で「**許可を追加**」をクリックし、「**ポリシーを直接アタッチする**」を選択します。
3. 先ほど作成したポリシー（`UserOgiriTrainingJobPolicy`）を検索し、選択して「**次へ**」をクリックし、「**許可を追加**」をクリックします。

### 5. S3 バケットの設定

#### 手順:

1. **S3**ダッシュボードに移動し、デフォルトの設定で`ogiri-training-data-bucket`を作成します。
2. バケットの「**プロパティ**」タブを開き、下にスクロールして「**イベント通知**」セクションを見つけます。
3. 「**イベント通知を追加**」をクリックし、名前を入力します（例: `OgiriCSVUploadEvent`）。
4. 「**イベントタイプ**」で「**すべてのオブジェクト作成イベント**」を選択します。
5. 「**プレフィックス**」と「**サフィックス**」を設定して、CSV ファイルに限定します（例: `サフィックス: .csv`）。
6. 「**Lambda 関数**」セクションで、「**Lambda 関数の選択**」から先ほど作成した関数（`StartOgiriTrainingJob`）を選択します。
7. 「**保存**」をクリックします。

### 6. DynamoDB テーブルの作成

#### 手順:

1. **DynamoDB**ダッシュボードに移動し、「**テーブルを作成**」をクリックします。
2. テーブル名を入力します（例: `OgiriTrainingDataTable`）。
3. **プライマリキー**として、以下を設定します:
   - **パーティションキー**: `ImageKey`（タイプ：文字列）
   - **ソートキー**: `ExpectedResult`（タイプ：文字列）
4. **その他の設定**の**プロビジョニングモード**は「オンデマンドキャパシティーモード」を選択します
   （または、必要に応じてプロビジョンドキャパシティーモードを選択し、読み取り/書き込みキャパシティーユニットを設定します）。
5. 「**テーブルの作成**」をクリックします。

| 設定項目               | 設定値                           |
| ---------------------- | -------------------------------- |
| テーブル名             | OgiriTrainingDataTable           |
| パーティションキー     | ImageKey（タイプ：文字列）       |
| ソートキー             | ExpectedResult（タイプ：文字列） |
| プロビジョニングモード | オンデマンドキャパシティーモード |

### 7. SageMaker の設定

#### 7.1. トレーニングスクリプトの準備

`training_script.py`を作成する

<details><summary>詳細をクリック</summary>

```python
import logging
import boto3
import json
import os
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pack_padded_sequence
from PIL import Image
from io import BytesIO
import requests
import argparse

# CloudWatch Logsの設定
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 定数の定義
EMBEDDING_DIM = 256
HIDDEN_DIM = 512
VOCAB_SIZE = 5000  # 語彙サイズは必要に応じて調整
MAX_SEQ_LENGTH = 20

# 画像特徴抽出モデル（ResNet）にランダム性を追加
class EncoderCNN(nn.Module):
    def __init__(self, embed_size):
        super(EncoderCNN, self).__init__()
        resnet = models.resnet50(pretrained=True)  # ResNetモデルの読み込み
        modules = list(resnet.children())[:-1]  # 最後の全結合層を除去
        self.resnet = nn.Sequential(*modules)
        self.linear = nn.Linear(resnet.fc.in_features, embed_size)  # 埋め込み層
        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)  # バッチ正規化層
        self.dropout = nn.Dropout(p=0.5)  # Dropout層でランダム性を追加

    def forward(self, images):
        with torch.no_grad():  # 勾配計算をしない
            features = self.resnet(images)  # ResNetで特徴抽出
        features = features.reshape(features.size(0), -1)
        features = self.bn(self.linear(features))  # 埋め込み層とバッチ正規化
        features = self.dropout(features)  # Dropoutを適用
        return features

# キャプション生成モデル（LSTM）
class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):
        super(DecoderRNN, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)  # 埋め込み層
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)  # LSTM層
        self.linear = nn.Linear(hidden_size, vocab_size)  # 全結合層
        self.max_seg_length = MAX_SEQ_LENGTH  # 最大シーケンス長

    def forward(self, features, captions):
        embeddings = self.embed(captions)  # キャプションを埋め込みに変換
        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)  # 特徴とキャプションを結合
        hiddens, _ = self.lstm(embeddings)  # LSTMで処理
        outputs = self.linear(hiddens)  # 全結合層で出力
        return outputs

    def sample(self, features, states=None):
        sampled_ids = []
        inputs = features.unsqueeze(1)
        for i in range(self.max_seg_length):
            hiddens, states = self.lstm(inputs, states)  # LSTMで処理
            outputs = self.linear(hiddens.squeeze(1))  # 全結合層で出力
            _, predicted = outputs.max(1)  # 最も確率の高い単語を選択
            sampled_ids.append(predicted)
            inputs = self.embed(predicted)  # 埋め込みに変換
            inputs = inputs.unsqueeze(1)  # 次のLSTM入力の形に変換
        sampled_ids = torch.stack(sampled_ids, 1)  # サンプルIDをスタック
        return sampled_ids

# データセットクラスの定義
class OgiriDataset(Dataset):
    def __init__(self, dynamodb_table_name, s3_client, dynamodb_client):
        self.dynamodb_table_name = dynamodb_table_name
        self.s3_client = s3_client
        self.dynamodb_client = dynamodb_client
        self.data = self._load_data_from_dynamodb()  # DynamoDBからデータをロード
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),  # 画像サイズの変更
            transforms.ToTensor(),  # テンソルに変換
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # 正規化
        ])
        logger.info(f"DynamoDBから{len(self.data)}個のデータをロードしました。")

    def _load_data_from_dynamodb(self):
        paginator = self.dynamodb_client.get_paginator('scan')
        response_iterator = paginator.paginate(TableName=self.dynamodb_table_name)
        data = []
        for page in response_iterator:
            data.extend(page['Items'])
        return data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        image_url = item['ImageUrl']['S']
        response = requests.get(image_url)  # 画像をダウンロード
        image = Image.open(BytesIO(response.content)).convert('RGB')
        image = self.transform(image)  # 画像を変換
        expected_result = item['ExpectedResult']['S']
        labels = item['Labels']['SS']
        return image, expected_result, labels

# トレーニング関数
def train(args):
    dynamodb_table_name = os.environ['DYNAMODB_TABLE_NAME']
    s3_client = boto3.client('s3')
    dynamodb_client = boto3.client('dynamodb')

    batch_size = args.batch_size
    num_epochs = args.epochs
    learning_rate = args.learning_rate

    dataset = OgiriDataset(dynamodb_table_name, s3_client, dynamodb_client)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)  # データローダーの設定

    # モデルの定義
    encoder = EncoderCNN(EMBEDDING_DIM)
    decoder = DecoderRNN(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    encoder.to(device)
    decoder.to(device)

    # 最適化関数と損失関数の定義
    criterion = nn.CrossEntropyLoss()
    params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())
    optimizer = torch.optim.Adam(params, lr=learning_rate)

    # トレーニング開始
    logger.info("トレーニングを開始します")
    for epoch in range(num_epochs):
        logger.info(f"Epoch {epoch+1}/{num_epochs} started")
        for i, (images, captions, labels) in enumerate(dataloader):
            images = images.to(device)
            captions = captions.to(device)

            # 前方伝播
            features = encoder(images)
            outputs = decoder(features, captions)
            targets = pack_padded_sequence(captions, lengths=[len(caption) for caption in captions], batch_first=True)[0]
            loss = criterion(outputs.view(-1, outputs.size(2)), targets)

            # 逆伝播と最適化
            decoder.zero_grad()
            encoder.zero_grad()
            loss.backward()
            optimizer.step()

            if i % 100 == 0:
                logger.info(f"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(dataloader)}], Loss: {loss.item():.4f}")

    # モデルの保存
    torch.save(encoder.state_dict(), '/opt/ml/model/encoder.ckpt')
    torch.save(decoder.state_dict(), '/opt/ml/model/decoder.ckpt')
    logger.info("トレーニングが完了し、モデルを保存しました")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--learning_rate', type=float, default=0.001)
    args = parser.parse_args()
    train(args)
```

</details>

#### 7.2. トレーニングスクリプトを S3 にアップロード

1. AWS マネジメントコンソールで「S3」サービスに移動します。
2. アップロードしたいバケット（例: `ogiri-training-data-bucket`）を選択します。
3. 「オブジェクトをアップロード」をクリックし、`training_script.py`をアップロードします。

### 8. トレーニングジョブ開始する Lambda 関数の作成

1. **Lambda 関数の作成**：
 
   - Lambda ダッシュボードに移動し、「関数の作成」をクリックします。
   - 「一から作成」を選択し、関数名（例: `StartOgiriTrainingJob`）を入力し、ランタイムを Python 3.8 に設定します。
   - 実行ロールには「既存のロールを使用する」を選択し、`LambdaStartOgiriTrainingJobRole`を選択します。
   - 「関数の作成」をクリックします。

2. **Lambda 関数にコードを追加**：

<details><summary>詳細をクリック</summary>

```python
import json
import boto3
import csv
import requests
from botocore.exceptions import NoCredentialsError, ClientError
import logging

# CloudWatch Logsの設定
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

s3 = boto3.client('s3')
rekognition = boto3.client('rekognition')
dynamodb = boto3.resource('dynamodb')
sagemaker = boto3.client('sagemaker')

def lambda_handler(event, context):
    try:
        # イベントからバケット名とオブジェクトキーを取得
        bucket = event.get('bucket')
        key = event.get('key')

        if not bucket or not key:
            raise ValueError("バケットとキーはイベントで指定してください。")

        # S3からCSVファイルを取得
        csv_file = s3.get_object(Bucket=bucket, Key=key)
        csv_content = csv_file['Body'].read().decode('utf-8').splitlines()
        csv_reader = csv.reader(csv_content)

        # DynamoDBに学習用のデータを登録する
        table = dynamodb.Table('OgiriTrainingDataTable')

        logger.info(f"学習用のデータの登録を開始します。")
        for row in csv_reader:
            image_url, expected_result = row

            try:
                image_name = image_url.split('/')[-1]
                image_key = f"images/{image_name}"  # images ディレクトリに配置

                # DynamoDBに既にデータが存在するか確認
                response = table.get_item(Key={'ImageKey': image_key, 'ExpectedResult': expected_result})
                if 'Item' in response:
                    logger.info(f"{image_url}は既に処理されています。")
                    continue

                # Rekognitionを使用して画像を分析する
                rekognition_labels = None

                # 同じ画像が存在するかを確認する
                labels_response = table.query(
                    KeyConditionExpression=boto3.dynamodb.conditions.Key('ImageKey').eq(image_key)
                )

                # S3のURLを生成
                s3_url = f"https://{bucket}.s3.amazonaws.com/{image_key}"

                # 同じ画像が存在する場合は、DynamoDBの分析結果を使用する
                if labels_response['Items']:
                    rekognition_labels = labels_response['Items'][0]['Labels']
                else:
                    # 画像が存在しない場合は登録する

                    # グローバルなURLから画像をダウンロード
                    image_data = requests.get(image_url).content

                    # ダウンロードした画像をS3にアップロード
                    s3.put_object(Bucket=bucket, Key=image_key, Body=image_data)
                    logger.info(f"S3に{image_key}を登録しました。")

                    # Rekognitionを使用して画像を分析
                    rekognition_response = rekognition.detect_labels(
                        Image={'S3Object': {'Bucket': bucket, 'Name': image_key}},
                        MaxLabels=10
                    )
                    rekognition_labels = [label['Name'] for label in rekognition_response['Labels']]

                # DynamoDBに画像URLと期待結果を保存
                table = dynamodb.Table('OgiriTrainingDataTable')
                table.put_item(
                    Item={
                        'ImageKey': image_key,
                        'ExpectedResult': expected_result,
                        'ImageUrl': s3_url,
                        'Labels': labels
                    }
                )
                logger.info(f"{image_url}をDBに登録しました。")

            except requests.exceptions.RequestException as e:
                logger.error(f"{image_url}からの画像のダウロードに失敗しました: {str(e)}")
            except ClientError as e:
                logger.error(f"{image_url}の登録に失敗しました: {str(e)}")

        # SageMakerトレーニングジョブの作成
                # SageMakerトレーニングジョブの作成
        training_job_name = 'ogiri-training-job'
        response = sagemaker.create_training_job(
            TrainingJobName=training_job_name,
            HyperParameters={
                'batch_size': '32',
                'epochs': '10',
                'learning_rate': '0.001'
            },
            AlgorithmSpecification={
                'TrainingImage': '763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.6.0-cpu-py36-ubuntu16.04',
                'MetricDefinitions': [
                    {'Name': 'validation:error', 'Regex': 'validation:error=(.*)'}
                ],
                'TrainingInputMode': 'File',
                'EnableSageMakerMetricsTimeSeries': True,
                'ScriptMode': 'SageMaker',
                'TrainingScript': 's3://ogiri-training-data-bucket/training_script.py'
            },
            RoleArn='arn:aws:iam::765231401377:role/SageMakerOgiriTrainingJobRole',
            InputDataConfig=[
                {
                    'ChannelName': 'training',
                    'DataSource': {
                        'S3DataSource': {
                            'S3DataType': 'S3Prefix',
                            'S3Uri': 's3://ogiri-training-data-bucket/training_data.json',
                            'S3DataDistributionType': 'FullyReplicated'
                        }
                    },
                    'ContentType': 'application/json',
                    'InputMode': 'File'
                }
            ],
            OutputDataConfig={
                'S3OutputPath': 's3://ogiri-training-data-bucket/output/'
            },
            ResourceConfig={
                'InstanceType': 'ml.m5.large',
                'InstanceCount': 1,
                'VolumeSizeInGB': 50
            },
            StoppingCondition={
                'MaxRuntimeInSeconds': 86400
            },
            Environment={
                'DYNAMODB_TABLE_NAME': 'OgiriTrainingDataTable'
            }
        )

        logger.info("トレーニングジョブの開始に成功しました。")

        return {
            'statusCode': 200,
            'body': json.dumps('Tトレーニングジョブの開始に成功しました。')
        }
    except NoCredentialsError:
        logger.error("権限がないです。")
        return {
            'statusCode': 403,
            'body': '権限がないです。'
        }
    except Exception as e:
        logger.error(f"予期せぬ例外: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps(f"予期せぬ例外: {str(e)}")
        }
```

</details>

1. **テストイベントの設定**：
   - 「テスト」ボタンをクリックし、テストイベントを作成します。以下のような JSON を使用します：
     - [トレーニングジョブ終了後の lambda 関数の作成](#10-トレーニングジョブ終了後のlambda関数の作成)完了後にテストする

```json
{
  "bucket": "ogiri-training-data-bucket",
  "key": "train_data.csv"
}
```

### 9. CloudWatch Event Rule を設定する

#### 9.1. CloudWatch Event ルールの作成

1. AWS Management Console で「CloudWatch」を開きます。
2. 左側のナビゲーションペインで「**イベント**」を展開し、「**ルール**」をクリックします。
3. 「**ルールの作成**」ボタンをクリックします。

#### 9.2. ルールの詳細の設定

1. **ルール名と説明**を入力します：

   - 「**名前**」に `OgiriTrainingJobCompletionRule` などの名前を入力します。
   - 「**説明**」にルールの目的を簡潔に記載します（例：`Trigger Lambda function when SageMaker training job completes`）。
   - 「**イベントバス**」は「**default**」に設定し「**選択したイベントバスでルールを有効にする**」をオンにします。
   - 「**ルールタイプ**」を「**イベントパターンを持つルール**」に設定します。

2. **イベントパターン**を設定します：

   - 「**イベントソース**」は 「**AWS イベントまたは EventBridge パートナーイベント**」 などの名前を入力します。
   - 「**サンプルイベント - オプション**」はデフォルトの値を設定します。
   - 「**作成のメソッド**」は「**カスタムパターン (JSON エディタ)**」 を選択します。

3. 以下のイベントパターンを入力します：

```json
{
  "source": ["aws.sagemaker"],
  "detail-type": ["SageMaker Training Job State Change"],
  "detail": {
    "TrainingJobName": ["ogiri-training-job"],
    "TrainingJobStatus": ["Completed"]
  }
}
```

5. **ターゲット**の設定

   - 「**Targets**」セクションで「**Add target**」をクリックします。
   - 「**Target**」ドロップダウンメニューで「**Lambda 関数**」を選択します。
   - 「**Function**」ドロップダウンメニューで、ターゲットにする Lambda 関数（例：`EndOgiriTrainingJob`）を選択します。

6. **ルール**の作成

   - すべての設定を確認し、「**ルールの作成**」ボタンをクリックします。
   - 設定内容を確認し、「**ルールの作成**」をクリックしてルールを作成します。

### 10. トレーニングジョブ終了後の Lambda 関数の作成

1. **Lambda 関数の作成**：
 
   - Lambda ダッシュボードに移動し、「関数の作成」をクリックします。
   - 「一から作成」を選択し、関数名（例: `EndOgiriTrainingJob`）を入力し、ランタイムを Python 3.8 に設定します。
   - 実行ロールには「既存のロールを使用する」を選択し、`LambdaEndOgiriTrainingJobRole`を選択します。
   - 「関数の作成」をクリックします。

2. **Lambda 関数にコードを追加**：
   - 作成した Lambda 関数に以下のコードを追加します。

<details><summary>詳細をクリック</summary>

```python
import json
import boto3
import logging

# CloudWatch Logsの設定
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

sagemaker = boto3.client('sagemaker')

def lambda_handler(event, context):
    try:
        training_job_name = event['detail']['TrainingJobName']

        # トレーニングジョブの完了を確認
        if event['detail']['TrainingJobStatus'] != 'Completed':
            logger.error(f"トレーニングジョブ{training_job_name}が正常に完了しませんでした。")
            return {
                'statusCode': 400,
                'body': json.dumps('トレーニングが正常に完了しませんでした。')
            }

        # モデルの作成
        model_name = 'ogiri-model'
        sagemaker.create_model(
            ModelName=model_name,
            PrimaryContainer={
                'Image': '763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.6.0-cpu-py36-ubuntu16.04',
                'ModelDataUrl': f"s3://ogiri-training-data-bucket/output/{training_job_name}/output/model.tar.gz"
            },
            ExecutionRoleArn='arn:aws:iam::765231401377:role/SageMakerOgiriTrainingJobRole'
        )

        logger.info("モデルの作成に成功しました。")

        # エンドポイント構成の作成
        endpoint_config_name = 'ogiri-endpoint-config'
        sagemaker.create_endpoint_config(
            EndpointConfigName=endpoint_config_name,
            ProductionVariants=[
                {
                    'VariantName': 'AllTraffic',
                    'ModelName': model_name,
                    'InstanceType': 'ml.m5.large',
                    'InitialInstanceCount': 1
                }
            ]
        )

        logger.info("エンドポイント構成の作成に成功しました。")

        # エンドポイントの作成
        endpoint_name = 'ogiri-endpoint'
        sagemaker.create_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=endpoint_config_name
        )

        logger.info("エンドポイントの作成に成功しました。")

        return {
            'statusCode': 200,
            'body': json.dumps('モデルとエンドポイントの作成成功しました。')
        }
    except Exception as e:
        logger.error(f"予期せぬ例外: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps(f"予期せぬ例外: {str(e)}")
        }
```

</details>